{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read libraries\n",
    "2. Data preparation\n",
    "\t- Re-split data into train and test\n",
    "3. Data vectorization (for train data)\n",
    "\t- Word tokenization\n",
    "\t- Word stemming\n",
    "\t- Word Lemmatization\n",
    "\t- Bag of Words (BoW) with train data\n",
    "\t- Tf - idf with train data\n",
    "4. Data vectorization (for test data)\n",
    "\t- Word tokenization\n",
    "\t- Word stemming\n",
    "\t- Word Lemmatization\n",
    "\t- BoW with test data\n",
    "\t- TF-IDF with test data\n",
    "5. Model Building: Sentiment Analysis\n",
    "\t- Splitting the Dataset into Train and Test set\n",
    "\t- Logistic Regression\n",
    "\t- Logistic Regression (2)\n",
    "\t- Support Vector Machine (SVM)\n",
    "\t- Gaussian NB classifier\n",
    "\t- MultinomialNB classifier\n",
    "\t- Xgboost classifier\n",
    "\t- Decision Tree\n",
    "\t- Random Forest\n",
    "\t- Deep Learning Classification (RNN-LSTM)\n",
    "\t- Using Vader Pre-trained model\n",
    "6. Data prediction\n",
    "7. Save Model\n",
    "8. Load Model\n",
    "9. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Read libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# for nlp\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "\n",
    "# for stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "stemming = PorterStemmer()\n",
    "\n",
    "# for Lemmatizing\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizing = WordNetLemmatizer()\n",
    "\n",
    "# for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# for machine learning\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "# from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
    "\n",
    "# save and load models\n",
    "import pickle\n",
    "\n",
    "# import warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This includes**:\n",
    "    - Re-split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>numerics</th>\n",
       "      <th>upper_case</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>dysfunctional selfish drags kids dysfunction #run</td>\n",
       "      <td>run</td>\n",
       "      <td>21</td>\n",
       "      <td>102</td>\n",
       "      <td>4.555556</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>thanks #lyft credit cause offer wheelchair van...</td>\n",
       "      <td>lyft  disapointed  getthanked</td>\n",
       "      <td>22</td>\n",
       "      <td>122</td>\n",
       "      <td>5.315789</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>majesty</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>#model</td>\n",
       "      <td>model</td>\n",
       "      <td>17</td>\n",
       "      <td>86</td>\n",
       "      <td>4.928571</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide society #motivation</td>\n",
       "      <td>motivation</td>\n",
       "      <td>8</td>\n",
       "      <td>39</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49154</th>\n",
       "      <td>49155</td>\n",
       "      <td>NaN</td>\n",
       "      <td>thought factory: left-right polarisation! #tru...</td>\n",
       "      <td>thought factory left right polarisation #trump...</td>\n",
       "      <td>trump  uselections      leadership  politics ...</td>\n",
       "      <td>13</td>\n",
       "      <td>108</td>\n",
       "      <td>8.727273</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49155</th>\n",
       "      <td>49156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>feeling like a mermaid ð #hairflip #neverre...</td>\n",
       "      <td>feeling mermaid #hairflip #neverready #formal ...</td>\n",
       "      <td>hairflip  neverready  formal  wedding  gown  ...</td>\n",
       "      <td>15</td>\n",
       "      <td>96</td>\n",
       "      <td>6.307692</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49156</th>\n",
       "      <td>49157</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#hillary #campaigned today in #ohio((omg)) &amp;am...</td>\n",
       "      <td>#hillary #campaigned #ohio used words assets l...</td>\n",
       "      <td>hillary  campaigned  ohio  omg    clinton  ra...</td>\n",
       "      <td>20</td>\n",
       "      <td>145</td>\n",
       "      <td>7.411765</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49157</th>\n",
       "      <td>49158</td>\n",
       "      <td>NaN</td>\n",
       "      <td>happy, at work conference: right mindset leads...</td>\n",
       "      <td>work conference right mindset leads culture de...</td>\n",
       "      <td>work  mindset</td>\n",
       "      <td>15</td>\n",
       "      <td>104</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49158</th>\n",
       "      <td>49159</td>\n",
       "      <td>NaN</td>\n",
       "      <td>my   song \"so glad\" free download!  #shoegaze ...</td>\n",
       "      <td>song glad free download #newmusic #newsong</td>\n",
       "      <td>shoegaze  newmusic  newsong</td>\n",
       "      <td>12</td>\n",
       "      <td>64</td>\n",
       "      <td>5.888889</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49159 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  label                                              tweet  \\\n",
       "0          1    0.0   @user when a father is dysfunctional and is s...   \n",
       "1          2    0.0  @user @user thanks for #lyft credit i can't us...   \n",
       "2          3    0.0                                bihday your majesty   \n",
       "3          4    0.0  #model   i love u take with u all the time in ...   \n",
       "4          5    0.0             factsguide: society now    #motivation   \n",
       "...      ...    ...                                                ...   \n",
       "49154  49155    NaN  thought factory: left-right polarisation! #tru...   \n",
       "49155  49156    NaN  feeling like a mermaid ð #hairflip #neverre...   \n",
       "49156  49157    NaN  #hillary #campaigned today in #ohio((omg)) &am...   \n",
       "49157  49158    NaN  happy, at work conference: right mindset leads...   \n",
       "49158  49159    NaN  my   song \"so glad\" free download!  #shoegaze ...   \n",
       "\n",
       "                                              tidy_tweet  \\\n",
       "0      dysfunctional selfish drags kids dysfunction #run   \n",
       "1      thanks #lyft credit cause offer wheelchair van...   \n",
       "2                                                majesty   \n",
       "3                                                 #model   \n",
       "4                         factsguide society #motivation   \n",
       "...                                                  ...   \n",
       "49154  thought factory left right polarisation #trump...   \n",
       "49155  feeling mermaid #hairflip #neverready #formal ...   \n",
       "49156  #hillary #campaigned #ohio used words assets l...   \n",
       "49157  work conference right mindset leads culture de...   \n",
       "49158         song glad free download #newmusic #newsong   \n",
       "\n",
       "                                                 hashtag  word_count  \\\n",
       "0                                                    run          21   \n",
       "1                          lyft  disapointed  getthanked          22   \n",
       "2                                                    NaN           5   \n",
       "3                                                  model          17   \n",
       "4                                             motivation           8   \n",
       "...                                                  ...         ...   \n",
       "49154   trump  uselections      leadership  politics ...          13   \n",
       "49155   hairflip  neverready  formal  wedding  gown  ...          15   \n",
       "49156   hillary  campaigned  ohio  omg    clinton  ra...          20   \n",
       "49157                                      work  mindset          15   \n",
       "49158                        shoegaze  newmusic  newsong          12   \n",
       "\n",
       "       char_count  avg_word  stopwords  hashtags  numerics  upper_case  \n",
       "0             102  4.555556         10         1         0           0  \n",
       "1             122  5.315789          5         3         0           0  \n",
       "2              21  5.666667          1         0         0           0  \n",
       "3              86  4.928571          5         1         0           0  \n",
       "4              39  8.000000          1         1         0           0  \n",
       "...           ...       ...        ...       ...       ...         ...  \n",
       "49154         108  8.727273          0         6         0           0  \n",
       "49155          96  6.307692          1         7         0           0  \n",
       "49156         145  7.411765          3         5         0           0  \n",
       "49157         104  7.500000          2         2         0           0  \n",
       "49158          64  5.888889          1         3         0           0  \n",
       "\n",
       "[49159 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data\n",
    "path = ''\n",
    "df = pd.read_csv(path + 'trump_analyzed_df.csv')\n",
    "del df['Unnamed: 0']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "path = ''\n",
    "df = pd.read_csv(path + 'trump_analyzed_df.csv')\n",
    "del df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'script', 'tidy_script', 'hashtag', 'word_count', 'char_count',\n",
       "       'avg_word', 'stopwords', 'hashtags', 'numerics', 'upper_case'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Re-split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12 entries, 0 to 11\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   id           12 non-null     int64  \n",
      " 1   script       12 non-null     object \n",
      " 2   tidy_script  12 non-null     object \n",
      " 3   hashtag      0 non-null      float64\n",
      " 4   word_count   12 non-null     int64  \n",
      " 5   char_count   12 non-null     int64  \n",
      " 6   avg_word     12 non-null     float64\n",
      " 7   stopwords    12 non-null     int64  \n",
      " 8   hashtags     12 non-null     int64  \n",
      " 9   numerics     12 non-null     int64  \n",
      " 10  upper_case   12 non-null     int64  \n",
      "dtypes: float64(2), int64(7), object(2)\n",
      "memory usage: 1.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# check data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hashtag']='none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data based on label\n",
    "# train_df = df[0:31962]\n",
    "# test_df = df[31962:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After re-splitting**\n",
    "\n",
    "- train data: (31962, 3)\n",
    "- test data: (17197, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12 entries, 0 to 11\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   id           12 non-null     int64  \n",
      " 1   script       12 non-null     object \n",
      " 2   tidy_script  12 non-null     object \n",
      " 3   hashtag      12 non-null     object \n",
      " 4   word_count   12 non-null     int64  \n",
      " 5   char_count   12 non-null     int64  \n",
      " 6   avg_word     12 non-null     float64\n",
      " 7   stopwords    12 non-null     int64  \n",
      " 8   hashtags     12 non-null     int64  \n",
      " 9   numerics     12 non-null     int64  \n",
      " 10  upper_case   12 non-null     int64  \n",
      "dtypes: float64(1), int64(7), object(3)\n",
      "memory usage: 1.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# chek train dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important column here is **'tidy_tweet',** which has the pre-processed version of the tweets. We will do verctorization and modeling over this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 12 entries, 0 to 11\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   id           12 non-null     int64  \n",
      " 1   script       12 non-null     object \n",
      " 2   tidy_script  12 non-null     object \n",
      " 3   hashtag      12 non-null     object \n",
      " 4   word_count   12 non-null     int64  \n",
      " 5   char_count   12 non-null     int64  \n",
      " 6   avg_word     12 non-null     float64\n",
      " 7   stopwords    12 non-null     int64  \n",
      " 8   hashtags     12 non-null     int64  \n",
      " 9   numerics     12 non-null     int64  \n",
      " 10  upper_case   12 non-null     int64  \n",
      "dtypes: float64(1), int64(7), object(3)\n",
      "memory usage: 1.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# drop rows where tidy_tweet = null\n",
    "df = df[df['tidy_script'].notna()]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[['tidy_script']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tidy_script</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hello iowa congratulations iowa hawkers today ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>running many love marjorie help take house sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>grew gutted thing round question could could w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nation america mourn loss brave brilliant amer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>crowd tell goes wish show goes looked televisi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         tidy_script\n",
       "0  hello iowa congratulations iowa hawkers today ...\n",
       "1  running many love marjorie help take house sen...\n",
       "2  grew gutted thing round question could could w...\n",
       "3  nation america mourn loss brave brilliant amer...\n",
       "4  crowd tell goes wish show goes looked televisi..."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['word_count'] = df2['tidy_script'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7103.5"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.word_count.sum()/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate 7100 rows of new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [[hello, iowa, congratulations, iowa, hawkers]...\n",
       "1     [[running, many, love, marjorie, help], [many,...\n",
       "2     [[grew, gutted, thing, round, question], [gutt...\n",
       "3     [[nation, america, mourn, loss, brave], [ameri...\n",
       "4     [[crowd, tell, goes, wish, show], [tell, goes,...\n",
       "5     [[seen, vanity, president, seen, vanity], [van...\n",
       "6     [[charlie, introduction, beautiful, fearless, ...\n",
       "7     [[audience, matt, precedes, done, can], [matt,...\n",
       "8     [[broke, rolling, topic, elite, firms], [rolli...\n",
       "9     [[audience, audience, crosstalk, well, ohio], ...\n",
       "10    [[well, michael, congratulations, reelection, ...\n",
       "11    [[well, hello, can, miss, miss], [hello, can, ...\n",
       "Name: tidy_script, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['tidy_script'].apply(lambda x: [ x.split()[i:i+5] for i in range(len(x.split())-6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data vectorization (for train data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use textual data for predictive modeling, words need to then be encoded as integers, or floating-point values, for use as inputs in machine learning algorithms. This process is called feature extraction (or vectorization). This includes:\n",
    "\n",
    "- Word tokenization\n",
    "- Data normalization\n",
    "    - Word stemming\n",
    "    - Word Lemmatization\n",
    "- Bag of Words (BoW) with train data\n",
    "- Tf - idf with train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>script</th>\n",
       "      <th>tidy_script</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>numerics</th>\n",
       "      <th>upper_case</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\\n\\n\\n\\n \\nDonald Trump: (00:13)\\nHello, Iowa....</td>\n",
       "      <td>hello iowa congratulations iowa hawkers today ...</td>\n",
       "      <td>none</td>\n",
       "      <td>15295</td>\n",
       "      <td>86940</td>\n",
       "      <td>4.581675</td>\n",
       "      <td>5641</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>362</td>\n",
       "      <td>[hello, iowa, congratulations, iowa, hawkers, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\\n\\n\\n\\n \\nDonald Trump: (03:37)\\nWe have grea...</td>\n",
       "      <td>running many love marjorie help take house sen...</td>\n",
       "      <td>none</td>\n",
       "      <td>13511</td>\n",
       "      <td>76565</td>\n",
       "      <td>4.573238</td>\n",
       "      <td>5115</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>271</td>\n",
       "      <td>[running, many, love, marjorie, help, take, ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>\\n\\n\\n\\n \\nGreg Gutfeld: (00:05)\\nAll right. W...</td>\n",
       "      <td>grew gutted thing round question could could w...</td>\n",
       "      <td>none</td>\n",
       "      <td>1130</td>\n",
       "      <td>6763</td>\n",
       "      <td>4.575557</td>\n",
       "      <td>403</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>61</td>\n",
       "      <td>[grew, gutted, thing, round, question, could, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\\n\\n\\n\\n \\nDonald Trump: (00:00)\\nAs one natio...</td>\n",
       "      <td>nation america mourn loss brave brilliant amer...</td>\n",
       "      <td>none</td>\n",
       "      <td>321</td>\n",
       "      <td>1860</td>\n",
       "      <td>4.742236</td>\n",
       "      <td>136</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>[nation, america, mourn, loss, brave, brillian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>\\n\\n\\n\\n \\nDonald Trump: (08:53)\\nThank you. T...</td>\n",
       "      <td>crowd tell goes wish show goes looked televisi...</td>\n",
       "      <td>none</td>\n",
       "      <td>13628</td>\n",
       "      <td>77118</td>\n",
       "      <td>4.548464</td>\n",
       "      <td>5021</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>345</td>\n",
       "      <td>[crowd, tell, goes, wish, show, goes, looked, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                             script  \\\n",
       "0   0  \\n\\n\\n\\n \\nDonald Trump: (00:13)\\nHello, Iowa....   \n",
       "1   1  \\n\\n\\n\\n \\nDonald Trump: (03:37)\\nWe have grea...   \n",
       "2   2  \\n\\n\\n\\n \\nGreg Gutfeld: (00:05)\\nAll right. W...   \n",
       "3   3  \\n\\n\\n\\n \\nDonald Trump: (00:00)\\nAs one natio...   \n",
       "4   4  \\n\\n\\n\\n \\nDonald Trump: (08:53)\\nThank you. T...   \n",
       "\n",
       "                                         tidy_script hashtag  word_count  \\\n",
       "0  hello iowa congratulations iowa hawkers today ...    none       15295   \n",
       "1  running many love marjorie help take house sen...    none       13511   \n",
       "2  grew gutted thing round question could could w...    none        1130   \n",
       "3  nation america mourn loss brave brilliant amer...    none         321   \n",
       "4  crowd tell goes wish show goes looked televisi...    none       13628   \n",
       "\n",
       "   char_count  avg_word  stopwords  hashtags  numerics  upper_case  \\\n",
       "0       86940  4.581675       5641         0        43         362   \n",
       "1       76565  4.573238       5115         0        41         271   \n",
       "2        6763  4.575557        403         0         2          61   \n",
       "3        1860  4.742236        136         0         1           6   \n",
       "4       77118  4.548464       5021         0        51         345   \n",
       "\n",
       "                                               token  \n",
       "0  [hello, iowa, congratulations, iowa, hawkers, ...  \n",
       "1  [running, many, love, marjorie, help, take, ho...  \n",
       "2  [grew, gutted, thing, round, question, could, ...  \n",
       "3  [nation, america, mourn, loss, brave, brillian...  \n",
       "4  [crowd, tell, goes, wish, show, goes, looked, ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['token'] = df['tidy_script'].apply(lambda x: word_tokenize(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Word stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>script</th>\n",
       "      <th>tidy_script</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>numerics</th>\n",
       "      <th>upper_case</th>\n",
       "      <th>token</th>\n",
       "      <th>script_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\\n\\n\\n\\n \\nDonald Trump: (00:13)\\nHello, Iowa....</td>\n",
       "      <td>hello iowa congratulations iowa hawkers today ...</td>\n",
       "      <td>none</td>\n",
       "      <td>15295</td>\n",
       "      <td>86940</td>\n",
       "      <td>4.581675</td>\n",
       "      <td>5641</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>362</td>\n",
       "      <td>[hello, iowa, congratulations, iowa, hawkers, ...</td>\n",
       "      <td>hello iowa congratul iowa hawker today thrill ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\\n\\n\\n\\n \\nDonald Trump: (03:37)\\nWe have grea...</td>\n",
       "      <td>running many love marjorie help take house sen...</td>\n",
       "      <td>none</td>\n",
       "      <td>13511</td>\n",
       "      <td>76565</td>\n",
       "      <td>4.573238</td>\n",
       "      <td>5115</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>271</td>\n",
       "      <td>[running, many, love, marjorie, help, take, ho...</td>\n",
       "      <td>run mani love marjori help take hous send nanc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>\\n\\n\\n\\n \\nGreg Gutfeld: (00:05)\\nAll right. W...</td>\n",
       "      <td>grew gutted thing round question could could w...</td>\n",
       "      <td>none</td>\n",
       "      <td>1130</td>\n",
       "      <td>6763</td>\n",
       "      <td>4.575557</td>\n",
       "      <td>403</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>61</td>\n",
       "      <td>[grew, gutted, thing, round, question, could, ...</td>\n",
       "      <td>grew gut thing round question could could wife...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\\n\\n\\n\\n \\nDonald Trump: (00:00)\\nAs one natio...</td>\n",
       "      <td>nation america mourn loss brave brilliant amer...</td>\n",
       "      <td>none</td>\n",
       "      <td>321</td>\n",
       "      <td>1860</td>\n",
       "      <td>4.742236</td>\n",
       "      <td>136</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>[nation, america, mourn, loss, brave, brillian...</td>\n",
       "      <td>nation america mourn loss brave brilliant amer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>\\n\\n\\n\\n \\nDonald Trump: (08:53)\\nThank you. T...</td>\n",
       "      <td>crowd tell goes wish show goes looked televisi...</td>\n",
       "      <td>none</td>\n",
       "      <td>13628</td>\n",
       "      <td>77118</td>\n",
       "      <td>4.548464</td>\n",
       "      <td>5021</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>345</td>\n",
       "      <td>[crowd, tell, goes, wish, show, goes, looked, ...</td>\n",
       "      <td>crowd tell goe wish show goe look televis tele...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                             script  \\\n",
       "0   0  \\n\\n\\n\\n \\nDonald Trump: (00:13)\\nHello, Iowa....   \n",
       "1   1  \\n\\n\\n\\n \\nDonald Trump: (03:37)\\nWe have grea...   \n",
       "2   2  \\n\\n\\n\\n \\nGreg Gutfeld: (00:05)\\nAll right. W...   \n",
       "3   3  \\n\\n\\n\\n \\nDonald Trump: (00:00)\\nAs one natio...   \n",
       "4   4  \\n\\n\\n\\n \\nDonald Trump: (08:53)\\nThank you. T...   \n",
       "\n",
       "                                         tidy_script hashtag  word_count  \\\n",
       "0  hello iowa congratulations iowa hawkers today ...    none       15295   \n",
       "1  running many love marjorie help take house sen...    none       13511   \n",
       "2  grew gutted thing round question could could w...    none        1130   \n",
       "3  nation america mourn loss brave brilliant amer...    none         321   \n",
       "4  crowd tell goes wish show goes looked televisi...    none       13628   \n",
       "\n",
       "   char_count  avg_word  stopwords  hashtags  numerics  upper_case  \\\n",
       "0       86940  4.581675       5641         0        43         362   \n",
       "1       76565  4.573238       5115         0        41         271   \n",
       "2        6763  4.575557        403         0         2          61   \n",
       "3        1860  4.742236        136         0         1           6   \n",
       "4       77118  4.548464       5021         0        51         345   \n",
       "\n",
       "                                               token  \\\n",
       "0  [hello, iowa, congratulations, iowa, hawkers, ...   \n",
       "1  [running, many, love, marjorie, help, take, ho...   \n",
       "2  [grew, gutted, thing, round, question, could, ...   \n",
       "3  [nation, america, mourn, loss, brave, brillian...   \n",
       "4  [crowd, tell, goes, wish, show, goes, looked, ...   \n",
       "\n",
       "                                      script_stemmed  \n",
       "0  hello iowa congratul iowa hawker today thrill ...  \n",
       "1  run mani love marjori help take hous send nanc...  \n",
       "2  grew gut thing round question could could wife...  \n",
       "3  nation america mourn loss brave brilliant amer...  \n",
       "4  crowd tell goe wish show goe look televis tele...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Created one more columns tweet_stemmed it shows tweets' stemmed version\n",
    "df['script_stemmed'] = df['token'].apply(lambda x: ' '.join([stemming.stem(i) for i in x]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Word Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>script</th>\n",
       "      <th>tidy_script</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>numerics</th>\n",
       "      <th>upper_case</th>\n",
       "      <th>token</th>\n",
       "      <th>script_stemmed</th>\n",
       "      <th>script_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\\n\\n\\n\\n \\nDonald Trump: (00:13)\\nHello, Iowa....</td>\n",
       "      <td>hello iowa congratulations iowa hawkers today ...</td>\n",
       "      <td>none</td>\n",
       "      <td>15295</td>\n",
       "      <td>86940</td>\n",
       "      <td>4.581675</td>\n",
       "      <td>5641</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>362</td>\n",
       "      <td>[hello, iowa, congratulations, iowa, hawkers, ...</td>\n",
       "      <td>hello iowa congratul iowa hawker today thrill ...</td>\n",
       "      <td>hello iowa congratulation iowa hawker today th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\\n\\n\\n\\n \\nDonald Trump: (03:37)\\nWe have grea...</td>\n",
       "      <td>running many love marjorie help take house sen...</td>\n",
       "      <td>none</td>\n",
       "      <td>13511</td>\n",
       "      <td>76565</td>\n",
       "      <td>4.573238</td>\n",
       "      <td>5115</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>271</td>\n",
       "      <td>[running, many, love, marjorie, help, take, ho...</td>\n",
       "      <td>run mani love marjori help take hous send nanc...</td>\n",
       "      <td>running many love marjorie help take house sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>\\n\\n\\n\\n \\nGreg Gutfeld: (00:05)\\nAll right. W...</td>\n",
       "      <td>grew gutted thing round question could could w...</td>\n",
       "      <td>none</td>\n",
       "      <td>1130</td>\n",
       "      <td>6763</td>\n",
       "      <td>4.575557</td>\n",
       "      <td>403</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>61</td>\n",
       "      <td>[grew, gutted, thing, round, question, could, ...</td>\n",
       "      <td>grew gut thing round question could could wife...</td>\n",
       "      <td>grew gutted thing round question could could w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\\n\\n\\n\\n \\nDonald Trump: (00:00)\\nAs one natio...</td>\n",
       "      <td>nation america mourn loss brave brilliant amer...</td>\n",
       "      <td>none</td>\n",
       "      <td>321</td>\n",
       "      <td>1860</td>\n",
       "      <td>4.742236</td>\n",
       "      <td>136</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>[nation, america, mourn, loss, brave, brillian...</td>\n",
       "      <td>nation america mourn loss brave brilliant amer...</td>\n",
       "      <td>nation america mourn loss brave brilliant amer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>\\n\\n\\n\\n \\nDonald Trump: (08:53)\\nThank you. T...</td>\n",
       "      <td>crowd tell goes wish show goes looked televisi...</td>\n",
       "      <td>none</td>\n",
       "      <td>13628</td>\n",
       "      <td>77118</td>\n",
       "      <td>4.548464</td>\n",
       "      <td>5021</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>345</td>\n",
       "      <td>[crowd, tell, goes, wish, show, goes, looked, ...</td>\n",
       "      <td>crowd tell goe wish show goe look televis tele...</td>\n",
       "      <td>crowd tell go wish show go looked television t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                             script  \\\n",
       "0   0  \\n\\n\\n\\n \\nDonald Trump: (00:13)\\nHello, Iowa....   \n",
       "1   1  \\n\\n\\n\\n \\nDonald Trump: (03:37)\\nWe have grea...   \n",
       "2   2  \\n\\n\\n\\n \\nGreg Gutfeld: (00:05)\\nAll right. W...   \n",
       "3   3  \\n\\n\\n\\n \\nDonald Trump: (00:00)\\nAs one natio...   \n",
       "4   4  \\n\\n\\n\\n \\nDonald Trump: (08:53)\\nThank you. T...   \n",
       "\n",
       "                                         tidy_script hashtag  word_count  \\\n",
       "0  hello iowa congratulations iowa hawkers today ...    none       15295   \n",
       "1  running many love marjorie help take house sen...    none       13511   \n",
       "2  grew gutted thing round question could could w...    none        1130   \n",
       "3  nation america mourn loss brave brilliant amer...    none         321   \n",
       "4  crowd tell goes wish show goes looked televisi...    none       13628   \n",
       "\n",
       "   char_count  avg_word  stopwords  hashtags  numerics  upper_case  \\\n",
       "0       86940  4.581675       5641         0        43         362   \n",
       "1       76565  4.573238       5115         0        41         271   \n",
       "2        6763  4.575557        403         0         2          61   \n",
       "3        1860  4.742236        136         0         1           6   \n",
       "4       77118  4.548464       5021         0        51         345   \n",
       "\n",
       "                                               token  \\\n",
       "0  [hello, iowa, congratulations, iowa, hawkers, ...   \n",
       "1  [running, many, love, marjorie, help, take, ho...   \n",
       "2  [grew, gutted, thing, round, question, could, ...   \n",
       "3  [nation, america, mourn, loss, brave, brillian...   \n",
       "4  [crowd, tell, goes, wish, show, goes, looked, ...   \n",
       "\n",
       "                                      script_stemmed  \\\n",
       "0  hello iowa congratul iowa hawker today thrill ...   \n",
       "1  run mani love marjori help take hous send nanc...   \n",
       "2  grew gut thing round question could could wife...   \n",
       "3  nation america mourn loss brave brilliant amer...   \n",
       "4  crowd tell goe wish show goe look televis tele...   \n",
       "\n",
       "                                   script_lemmatized  \n",
       "0  hello iowa congratulation iowa hawker today th...  \n",
       "1  running many love marjorie help take house sen...  \n",
       "2  grew gutted thing round question could could w...  \n",
       "3  nation america mourn loss brave brilliant amer...  \n",
       "4  crowd tell go wish show go looked television t...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['script_lemmatized'] = df['token'].apply(lambda x: ' '.join([lemmatizing.lemmatize(i) for i in x]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Bag of Words (BoW) with train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CounterVectorization is a SciKitLearn library takes any text document and returns each unique word as a feature with the count of number of times that word occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_df=0.9, max_features=1000, min_df=2, stop_words='english')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "bow_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<12x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 6732 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bag-of-words stemmed\n",
    "trainbow_stem = bow_vectorizer.fit_transform(df['script_stemmed'])\n",
    "trainbow_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 2, 2, ..., 4, 0, 0],\n",
       "       [1, 1, 1, ..., 5, 0, 1],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 1, ..., 2, 0, 1],\n",
       "       [4, 2, 5, ..., 3, 0, 3],\n",
       "       [1, 0, 5, ..., 5, 3, 0]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainbow_stem.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<12x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 6710 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bow lemmatized\n",
    "trainbow_lemm = bow_vectorizer.fit_transform(df['script_lemmatized'])\n",
    "trainbow_lemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 2, 0, ..., 4, 0, 0],\n",
       "       [1, 1, 0, ..., 5, 0, 1],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 2, ..., 2, 0, 1],\n",
       "       [4, 5, 2, ..., 3, 0, 3],\n",
       "       [1, 2, 2, ..., 5, 3, 0]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainbow_lemm.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Why we need toarray()? This method converts the sparse matrix representation to a dense ndarray representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR TURN**\n",
    "\n",
    "Can you use the code above without using toarray() function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Tf - idf with train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF** works by penalizing the common words by assigning them lower weights while giving importance to words which are rare in the entire corpus but appear in good numbers in few documents.\n",
    "\n",
    "Let’s have a look at the important terms related to TF-IDF:\n",
    "\n",
    "TF = (Number of times term t appears in a document)/(Number of terms in the document)\n",
    "IDF = log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in.\n",
    "TF-IDF = TF*IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_df=0.9, max_features=1000, min_df=2, stop_words='english')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02140815, 0.0136249 , 0.01157825, ..., 0.02508921, 0.        ,\n",
       "        0.        ],\n",
       "       [0.00596989, 0.00759889, 0.00645743, ..., 0.03498193, 0.        ,\n",
       "        0.00907048],\n",
       "       [0.01779407, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.00801385, 0.00681006, ..., 0.01475689, 0.        ,\n",
       "        0.0095658 ],\n",
       "       [0.0259271 , 0.01650091, 0.03505562, ..., 0.02278888, 0.        ,\n",
       "        0.02954469],\n",
       "       [0.00662603, 0.        , 0.03583579, ..., 0.03882675, 0.03330764,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf-idf stemmed\n",
    "traintfidf_stem = tfidf_vectorizer.fit_transform(df['script_stemmed'])\n",
    "traintfidf_stem.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02193933, 0.01186553, 0.        , ..., 0.02571172, 0.        ,\n",
       "        0.        ],\n",
       "       [0.00600493, 0.00649534, 0.        , ..., 0.03518728, 0.        ,\n",
       "        0.00912373],\n",
       "       [0.01817601, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.00670454, 0.02077182, ..., 0.01452823, 0.        ,\n",
       "        0.00941758],\n",
       "       [0.02499032, 0.03378902, 0.02093685, ..., 0.02196549, 0.        ,\n",
       "        0.02847721],\n",
       "       [0.00662474, 0.01433153, 0.02220078, ..., 0.03881919, 0.03330116,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf-idf lemmatized\n",
    "traintfidf_lemm = tfidf_vectorizer.fit_transform(df['script_lemmatized'])\n",
    "traintfidf_lemm.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data vectorization (for test data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This includes:\n",
    "    - Word tokenization\n",
    "    - Word stemming\n",
    "    - Word Lemmatization\n",
    "    - BoW with test data\n",
    "    - TF-IDF with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Building: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now done with all the pre-modeling stages required to get the data in the proper form and shape. We will be building models on the datasets with different feature sets prepared in the earlier sections — Bag-of-Words, and TF-IDF vectors. We will use the following algorithms to build models:\n",
    "\n",
    "- Splitting the Dataset into Train and Test set\n",
    "- Logistic Regression\n",
    "- Logistic Regression (2)\n",
    "- Support Vector Machine (SVM)\n",
    "- Gaussian NB classifier\n",
    "- MultinomialNB classifier\n",
    "- Xgboost classifier\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- Deep Learning Classification\n",
    "- Vader Pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A Note on Evaluation Metrics**\n",
    "\n",
    "We will use the following evaluation metrics:\n",
    "\n",
    "\n",
    "1. **Accuracy score** is the rate of correct predictions.Out of every 100 predictions made, the model was correct 94 times. It is used when we want to know the number of correct predictions, which is, when the algorithm correctly predicts a type T when it is actually type T. It takes into account all the possible classes and how much we predicted correctly.The score should be as high as possible.\n",
    "\n",
    "2. **F1 Score** It is the weighted average of Precision and Recall is used when the datasets don't have an equal representation for each type that's being classified. Therefore, this score takes both false positives and false negatives into account. It is suitable for uneven class distribution problems. It is calculated as follows: F1 Score = 2 (Recall Precision) / (Recall + Precision)\n",
    "\n",
    "3. **A confusion matrix** is a table that is often used to describe the performance of a classification model (or “classifier”) on a set of test data for which the true values are known. Confusion matrix represents accurate predictions made along the diagonal of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "path=\"C:\\\\Users\\\\lenovo\\\\Tutorials\\\\03. Data Science\\\\DS images 2\\\\confusion-matrix.png\"\n",
    "display(Image.open(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics\n",
    "path=\"C:\\\\Users\\\\lenovo\\\\Tutorials\\\\03. Data Science\\\\DS images 2\\\\f1-score.jpg\"\n",
    "display(Image.open(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Splitting the Dataset into Train and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 1000)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traintfidf_lemm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=traintfidf_lemm #x: predictors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain,xtest,ytrain,ytest=train_test_split(X,y,test_size=.3,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will aplly an number of classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=LogisticRegression() # for lemmatized data\n",
    "lr.fit(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_lr=lr.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score : 0.9455175309678774\n",
      "f1 score : 0.4188129899216126\n",
      "[[8820  479]\n",
      " [  40  187]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.95      0.97      9299\n",
      "         1.0       0.28      0.82      0.42       227\n",
      "\n",
      "    accuracy                           0.95      9526\n",
      "   macro avg       0.64      0.89      0.70      9526\n",
      "weighted avg       0.98      0.95      0.96      9526\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy score :\", accuracy_score(predict_lr,ytest))\n",
    "\n",
    "# calculating the f1 score for the validation set\n",
    "print(\"f1 score :\", f1_score(predict_lr,ytest))\n",
    "\n",
    "print(confusion_matrix(predict_lr,ytest))\n",
    "print(classification_report(predict_lr,ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Logistic Regression (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1=traintfidf_stem # for stemmed data\n",
    "y1=train_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1train,x1test,y1train,y1test=train_test_split(X1,y1,test_size=.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr1=LogisticRegression()\n",
    "lr1.fit(x1train,y1train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_lr1=lr1.predict(x1test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy score\n",
    "print(\"accuracy score :\", accuracy_score(predict_lr1,y1test))\n",
    "\n",
    "# calculating the f1 score for the validation set\n",
    "print(\"f1 score :\", f1_score(predict_lr1,ytest))\n",
    "\n",
    "print(confusion_matrix(predict_lr1,y1test))\n",
    "print(classification_report(predict_lr1,y1test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc=SVC()\n",
    "svc.fit(xtrain,ytrain)\n",
    "predict_svc=svc.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy score\n",
    "print(\"accuracy score :\", accuracy_score(predict_svc,ytest))\n",
    "\n",
    "# calculating the f1 score for the validation set\n",
    "print(\"f1 score :\", f1_score(predict_svc,ytest))\n",
    "\n",
    "print(confusion_matrix(predict_svc,ytest))\n",
    "print(classification_report(predict_svc,ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Gaussian NB classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes** is a classification technique based on Bayes' Theorem. Bayes’ theorem is based conditional probability which states the likelihood the occurrence of event “A” given another event “B” has already happened. There are 3 type of Naïve Bayes:\n",
    "\n",
    "1. **Gaussian** -> The model assume that the data follows normal distribution and all our features are continuous.\n",
    "2. **Bernoulli** -> It assumes that all our features are binary such that they only take two values: 0s and 1s.\n",
    "3. **Multinomial** -> It assumes that the data has discreate value such as ratings between 1 to 5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb=GaussianNB()\n",
    "nb.fit(xtrain.toarray(),ytrain)\n",
    "predict_nb=nb.predict(xtest.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy score\n",
    "print(\"accuracy score :\", accuracy_score(predict_nb,ytest))\n",
    "\n",
    "# calculating the f1 score for the validation set\n",
    "print(\"f1 score :\", f1_score(predict_nb,ytest))\n",
    "\n",
    "print(confusion_matrix(predict_nb,ytest))\n",
    "print(classification_report(predict_nb,ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 MultinomialNB classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlnb = MultinomialNB()\n",
    "mlnb.fit(xtrain.toarray(),ytrain)\n",
    "predict_mlnb=mlnb.predict(xtest.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy score\n",
    "print(\"accuracy score :\", accuracy_score(predict_mlnb,ytest))\n",
    "\n",
    "# calculating the f1 score for the validation set\n",
    "print(\"f1 score :\", f1_score(predict_mlnb,ytest))\n",
    "\n",
    "print(confusion_matrix(predict_mlnb,ytest))\n",
    "print(classification_report(predict_mlnb,ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Xgboost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier()\n",
    "xgb.fit(xtrain.toarray(),ytrain)\n",
    "predict_xgb=xgb.predict(xtest.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy score\n",
    "print(\"accuracy score :\", accuracy_score(predict_xgb,ytest))\n",
    "\n",
    "# calculating the f1 score for the validation set\n",
    "print(\"f1 score :\", f1_score(predict_xgb,ytest))\n",
    "\n",
    "print(confusion_matrix(predict_xgb,ytest))\n",
    "print(classification_report(predict_xgb,ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "Xgboost classifier is the preferred classifier to use in data science competitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18392/3276174874.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mpredict_dt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    935\u001b[0m         \"\"\"\n\u001b[0;32m    936\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 937\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m    938\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    418\u001b[0m             )\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 420\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(xtrain.toarray(),ytrain)\n",
    "predict_dt = dt.predict(xtest.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy score\n",
    "print(\"accuracy score :\", accuracy_score(predict_dt,ytest))\n",
    "\n",
    "# calculating the f1 score for the validation set\n",
    "print(\"f1 score :\", f1_score(predict_dt,ytest))\n",
    "\n",
    "print(confusion_matrix(predict_dt,ytest))\n",
    "print(classification_report(predict_dt,ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.9 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22225, 1000), (22225,))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.shape, ytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(xtrain.toarray(),ytrain) # you can test with grid search methodology\n",
    "predict_rf = rf.predict(xtest.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score : 0.9517111064455175\n",
      "f1 score : 0.5568400770712909\n",
      "[[8777  377]\n",
      " [  83  289]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.96      0.97      9154\n",
      "         1.0       0.43      0.78      0.56       372\n",
      "\n",
      "    accuracy                           0.95      9526\n",
      "   macro avg       0.71      0.87      0.77      9526\n",
      "weighted avg       0.97      0.95      0.96      9526\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# accuracy score\n",
    "print(\"accuracy score :\", accuracy_score(predict_rf,ytest))\n",
    "\n",
    "# calculating the f1 score for the validation set\n",
    "print(\"f1 score :\", f1_score(predict_rf,ytest))\n",
    "\n",
    "print(confusion_matrix(predict_rf,ytest))\n",
    "print(classification_report(predict_rf,ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.10 Deep Learning Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding, SpatialDropout1D\n",
    "from keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 220\n",
    "tokenizer = Tokenizer(num_words = max_features, split = (' '))\n",
    "tokenizer.fit_on_texts(train_df['tweet'].values)\n",
    "X = tokenizer.texts_to_sequences(train_df['tweet'].values)\n",
    "\n",
    "# making all the tokens into same sizes using padding.\n",
    "X = pad_sequences(X, maxlen = max_features)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = train_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 64, input_length = X.shape[1], trainable=False))\n",
    "model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.50))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, Y,batch_size=1500,epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X)\n",
    "classes_x=np.argmax(prediction,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "score = accuracy_score(Y, classes_x)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.11 Using Vader Pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['score']=train_df['tweet'].apply(lambda tweet: sid.polarity_scores(tweet))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['compound']  = train_df['score'].apply(lambda score_dict: score_dict['compound'])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['comp_score'] = train_df['compound'].apply(lambda c: 1 if c >0 else 0)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "score = accuracy_score(train_df['label'], train_df['comp_score'])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR TURN**\n",
    "\n",
    "Can you suggest other classifiers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forst model** has given us the best performance so far in terms of F1-score and accuracy. Let’s try to do predictions using the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forst\n",
    "test_predict_rf = rf.predict(testtfidf_lemm)\n",
    "test_predict_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "      <th>token</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "      <th>tweet_lemmatized</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31962</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "      <td>[#, studiolife, #, aislife, #, requires, #, pa...</td>\n",
       "      <td># studiolif # aislif # requir # passion # dedi...</td>\n",
       "      <td># studiolife # aislife # requires # passion # ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31963</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "      <td>#white #supremacists everyone #birds #movie</td>\n",
       "      <td>[#, white, #, supremacists, everyone, #, birds...</td>\n",
       "      <td># white # supremacist everyon # bird # movi</td>\n",
       "      <td># white # supremacist everyone # bird # movie</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31964</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "      <td>safe ways heal #acne #altwaystoheal #healing</td>\n",
       "      <td>[safe, ways, heal, #, acne, #, altwaystoheal, ...</td>\n",
       "      <td>safe way heal # acn # altwaystoh # heal</td>\n",
       "      <td>safe way heal # acne # altwaystoheal # healing</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31965</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "      <td>cursed child book reservations already #harryp...</td>\n",
       "      <td>[cursed, child, book, reservations, already, #...</td>\n",
       "      <td>curs child book reserv alreadi # harrypott # p...</td>\n",
       "      <td>cursed child book reservation already # harryp...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31966</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "      <td>#bihday amazing hilarious #nephew ahmir uncle ...</td>\n",
       "      <td>[#, bihday, amazing, hilarious, #, nephew, ahm...</td>\n",
       "      <td># bihday amaz hilari # nephew ahmir uncl dave ...</td>\n",
       "      <td># bihday amazing hilarious # nephew ahmir uncl...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet  \\\n",
       "31962  31963  #studiolife #aislife #requires #passion #dedic...   \n",
       "31963  31964   @user #white #supremacists want everyone to s...   \n",
       "31964  31965  safe ways to heal your #acne!!    #altwaystohe...   \n",
       "31965  31966  is the hp and the cursed child book up for res...   \n",
       "31966  31967    3rd #bihday to my amazing, hilarious #nephew...   \n",
       "\n",
       "                                              tidy_tweet  \\\n",
       "31962  #studiolife #aislife #requires #passion #dedic...   \n",
       "31963        #white #supremacists everyone #birds #movie   \n",
       "31964       safe ways heal #acne #altwaystoheal #healing   \n",
       "31965  cursed child book reservations already #harryp...   \n",
       "31966  #bihday amazing hilarious #nephew ahmir uncle ...   \n",
       "\n",
       "                                                   token  \\\n",
       "31962  [#, studiolife, #, aislife, #, requires, #, pa...   \n",
       "31963  [#, white, #, supremacists, everyone, #, birds...   \n",
       "31964  [safe, ways, heal, #, acne, #, altwaystoheal, ...   \n",
       "31965  [cursed, child, book, reservations, already, #...   \n",
       "31966  [#, bihday, amazing, hilarious, #, nephew, ahm...   \n",
       "\n",
       "                                           tweet_stemmed  \\\n",
       "31962  # studiolif # aislif # requir # passion # dedi...   \n",
       "31963        # white # supremacist everyon # bird # movi   \n",
       "31964            safe way heal # acn # altwaystoh # heal   \n",
       "31965  curs child book reserv alreadi # harrypott # p...   \n",
       "31966  # bihday amaz hilari # nephew ahmir uncl dave ...   \n",
       "\n",
       "                                        tweet_lemmatized  label  \n",
       "31962  # studiolife # aislife # requires # passion # ...    0.0  \n",
       "31963      # white # supremacist everyone # bird # movie    0.0  \n",
       "31964     safe way heal # acne # altwaystoheal # healing    0.0  \n",
       "31965  cursed child book reservation already # harryp...    0.0  \n",
       "31966  # bihday amazing hilarious # nephew ahmir uncl...    0.0  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['label'] = test_predict_rf\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    16710\n",
       "1.0      349\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Prediction: Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase column width\n",
    "pd.set_option('max_colwidth', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32084</th>\n",
       "      <td>32085</td>\n",
       "      <td>@user @user @user   always, always, always somebody else's fault...  #bigot</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32096</th>\n",
       "      <td>32097</td>\n",
       "      <td>#rainbow over wall street   a good way to end training!! #officialrainbowspotterâ¦</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32107</th>\n",
       "      <td>32108</td>\n",
       "      <td>4u nonhockey people. hockeys babe ruth died. gordie howe was beyond myth and legend. hockey has lost mr. hockey.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32190</th>\n",
       "      <td>32191</td>\n",
       "      <td>@user so relaxed   #peaceful âºï¸</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32228</th>\n",
       "      <td>32229</td>\n",
       "      <td>@user @user it is still fucking bullshit that they are giving  #bigots a platform to spread their vile #hate.  #boyâ¦</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48847</th>\n",
       "      <td>48848</td>\n",
       "      <td>happy 16th anniversary pti! ð·ðºð¯â  #pilipinasteleserv #pti16thanniversary   #cowboygrillâ¦</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48863</th>\n",
       "      <td>48864</td>\n",
       "      <td>just saw #thelivingandthedead trailer @user looks soo good! ðð   #cantwait</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48898</th>\n",
       "      <td>48899</td>\n",
       "      <td>@user ha! good riddance! #blacklivesmatter</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48969</th>\n",
       "      <td>48970</td>\n",
       "      <td>i remember days ago i just wana. say thank you almighty godÂ¤Â¤back to back.++***+++*###  blessed friday to all my palz in nation wild.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49006</th>\n",
       "      <td>49007</td>\n",
       "      <td>@user ok, i love ya, but stop the #bs. #obama was only   about another bad #newsrepo on #terrorist #trump2016</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>349 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  \\\n",
       "32084  32085   \n",
       "32096  32097   \n",
       "32107  32108   \n",
       "32190  32191   \n",
       "32228  32229   \n",
       "...      ...   \n",
       "48847  48848   \n",
       "48863  48864   \n",
       "48898  48899   \n",
       "48969  48970   \n",
       "49006  49007   \n",
       "\n",
       "                                                                                                                                         tweet  \\\n",
       "32084                                                             @user @user @user   always, always, always somebody else's fault...  #bigot    \n",
       "32096                                                     #rainbow over wall street   a good way to end training!! #officialrainbowspotterâ¦    \n",
       "32107                       4u nonhockey people. hockeys babe ruth died. gordie howe was beyond myth and legend. hockey has lost mr. hockey.     \n",
       "32190                                                                                                      @user so relaxed   #peaceful âºï¸   \n",
       "32228                   @user @user it is still fucking bullshit that they are giving  #bigots a platform to spread their vile #hate.  #boyâ¦   \n",
       "...                                                                                                                                        ...   \n",
       "48847                                   happy 16th anniversary pti! ð·ðºð¯â  #pilipinasteleserv #pti16thanniversary   #cowboygrillâ¦    \n",
       "48863                                                         just saw #thelivingandthedead trailer @user looks soo good! ðð   #cantwait   \n",
       "48898                                                                                              @user ha! good riddance! #blacklivesmatter    \n",
       "48969  i remember days ago i just wana. say thank you almighty godÂ¤Â¤back to back.++***+++*###  blessed friday to all my palz in nation wild.   \n",
       "49006                            @user ok, i love ya, but stop the #bs. #obama was only   about another bad #newsrepo on #terrorist #trump2016   \n",
       "\n",
       "       label  \n",
       "32084    1.0  \n",
       "32096    1.0  \n",
       "32107    1.0  \n",
       "32190    1.0  \n",
       "32228    1.0  \n",
       "...      ...  \n",
       "48847    1.0  \n",
       "48863    1.0  \n",
       "48898    1.0  \n",
       "48969    1.0  \n",
       "49006    1.0  \n",
       "\n",
       "[349 rows x 3 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_pos = test_df[test_df['label'] == 1]\n",
    "prediction_pos = prediction_pos[['id','tweet','label']]\n",
    "prediction_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Prediction: Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31962</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedication #willpower   to find #newmaterialsâ¦</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31963</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to see the new â  #birdsâ #movie â and hereâs why</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31964</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystoheal #healthy   #healing!!</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31965</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for reservations already? if yes, where? if no, when? ððð   #harrypotter #pottermore #favorite</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31966</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew eli ahmir! uncle dave loves you and missesâ¦</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49154</th>\n",
       "      <td>49155</td>\n",
       "      <td>thought factory: left-right polarisation! #trump #uselections2016 #leadership #politics  #brexit #blm &amp;gt;3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49155</th>\n",
       "      <td>49156</td>\n",
       "      <td>feeling like a mermaid ð #hairflip #neverready #formal #wedding #gown #dresses #mermaid  â¦</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49156</th>\n",
       "      <td>49157</td>\n",
       "      <td>#hillary #campaigned today in #ohio((omg)) &amp;amp; used words like \"assets&amp;amp;liability\" never once did #clinton say thee(word) #radicalization</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49157</th>\n",
       "      <td>49158</td>\n",
       "      <td>happy, at work conference: right mindset leads to culture-of-development organizations    #work #mindset</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49158</th>\n",
       "      <td>49159</td>\n",
       "      <td>my   song \"so glad\" free download!  #shoegaze #newmusic #newsong</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16710 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  \\\n",
       "31962  31963   \n",
       "31963  31964   \n",
       "31964  31965   \n",
       "31965  31966   \n",
       "31966  31967   \n",
       "...      ...   \n",
       "49154  49155   \n",
       "49155  49156   \n",
       "49156  49157   \n",
       "49157  49158   \n",
       "49158  49159   \n",
       "\n",
       "                                                                                                                                                   tweet  \\\n",
       "31962                                                         #studiolife #aislife #requires #passion #dedication #willpower   to find #newmaterialsâ¦    \n",
       "31963                                               @user #white #supremacists want everyone to see the new â  #birdsâ #movie â and hereâs why     \n",
       "31964                                                                            safe ways to heal your #acne!!    #altwaystoheal #healthy   #healing!!    \n",
       "31965     is the hp and the cursed child book up for reservations already? if yes, where? if no, when? ððð   #harrypotter #pottermore #favorite   \n",
       "31966                                                        3rd #bihday to my amazing, hilarious #nephew eli ahmir! uncle dave loves you and missesâ¦    \n",
       "...                                                                                                                                                  ...   \n",
       "49154                                       thought factory: left-right polarisation! #trump #uselections2016 #leadership #politics  #brexit #blm &gt;3    \n",
       "49155                                                   feeling like a mermaid ð #hairflip #neverready #formal #wedding #gown #dresses #mermaid  â¦    \n",
       "49156  #hillary #campaigned today in #ohio((omg)) &amp; used words like \"assets&amp;liability\" never once did #clinton say thee(word) #radicalization      \n",
       "49157                                           happy, at work conference: right mindset leads to culture-of-development organizations    #work #mindset   \n",
       "49158                                                                                   my   song \"so glad\" free download!  #shoegaze #newmusic #newsong   \n",
       "\n",
       "       label  \n",
       "31962    0.0  \n",
       "31963    0.0  \n",
       "31964    0.0  \n",
       "31965    0.0  \n",
       "31966    0.0  \n",
       "...      ...  \n",
       "49154    0.0  \n",
       "49155    0.0  \n",
       "49156    0.0  \n",
       "49157    0.0  \n",
       "49158    0.0  \n",
       "\n",
       "[16710 rows x 3 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_neg = test_df[test_df['label'] == 0]\n",
    "prediction_neg = prediction_neg[['id','tweet','label']]\n",
    "prediction_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "# we give what ever name in fist line (the model will be stored in that name)\n",
    "# in second line we provide the name of our model (which is classifier in our case)\n",
    "\n",
    "import pickle\n",
    "RVC_filename = 'finalized_RFC_model.sav' # finalized_RFC_model: is the new model name\n",
    "pickle.dump(rf, open(RVC_filename, 'wb')) # rf: random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model will be saved in the current directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tfidf vectorizer\n",
    "# Save fit vectorizer and fit tfidftransformer, use in prediction\n",
    "\n",
    "tfidftransformer_path = 'tfidf-vectorizer.pkl' # tfidf-vectorizer is the new vectorizer name\n",
    "with open(tfidftransformer_path, 'wb') as fw:\n",
    "    pickle.dump(traintfidf_lemm, fw) # traintfidf_lemm: vectorizer name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR TURN**\n",
    "\n",
    "There is another library to handle Machine learning models, called **'joblib'.** Can you use it to do the same job as pickle here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) load logistic regression\n",
    "import pickle\n",
    "with open('finalized_RFC_model.sav', 'rb') as f:\n",
    "    rf = pickle.load(f)\n",
    "\n",
    "rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<31751x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 95447 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) Load Tfidf vectorizer\n",
    "import pickle\n",
    "tfidftransformer_path = 'tfidf-vectorizer.pkl'\n",
    "vectorizer = pickle.load(open(tfidftransformer_path, \"rb\"))\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31751, 1000)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.predict(X).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR TURN**\n",
    "\n",
    "Can you apply this model to new unseen twitter data? Don't forget to pre-process and vectorize data before injecting it into the model!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module work, Logistic Regression, Support Vector Classifier, Gaussian NB, MultinomialNB, Decision Tree, Random Forest and XGBoost classifiers are used to perform Twitter sentiment analysis, out of these algorithms Random Forest classifier works best in terms of accuracy and F1 evaluation measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
